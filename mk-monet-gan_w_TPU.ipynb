{"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":21755,"databundleVersionId":1475600,"sourceType":"competition"}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Monet Style Transfer with GANs Kaggle Mini-Project\n\n## Brief description of the problem and data\n\nIn this project, the goal is to build a GAN that generates 7,000 to 10,000 Monet-style images.\n\nThe compeition uses a modified FID calculation to see if data science, in the form of GANs, can trick classifiers into believing that we have created a true Monet. That’s the challenge we will take on!\n\nA GAN consists of at least two neural networks: a generator model and a discriminator model. The generator is a neural network that creates the images. For our competition, you should generate images in the style of Monet. This generator is trained using a discriminator.\n\nThe two models will oppose each other, with the generator trying to trick the discriminator, and the discriminator trying to accurately classify the real vs. generated images.\n\nI have utilized the training notebook to learn how a CycleGAN should be implemented and then attempted to improve on that. \nhttps://www.kaggle.com/code/amyjang/monet-cyclegan-tutorial","metadata":{}},{"cell_type":"markdown","source":"## Exploratory Data Analysis\n\nIn this project we are going to use CycleGAN for the style tranfer. Below the packages necessary for building the GAN model are installed. We will use tensorflow keras to train the generative model.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport re, os, shutil\nfrom glob import glob\nimport tqdm\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, Model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    raise BaseException('ERROR: Not connected to a TPU runtime; please see the Kaggle instructions.')\n\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\nstrategy = tf.distribute.experimental.TPUStrategy(tpu)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's read the `tfrecords` and create a `tensorflow` `ZipDataset` by combining the photo and monte style images. This is necessary because the files are not perfectly matched and they are a different size, 300 vs 7000. ","metadata":{}},{"cell_type":"code","source":"def load_dataset(filenames, labeled=True, ordered=False, autotune=tf.data.experimental.AUTOTUNE):\n    dataset = tf.data.TFRecordDataset(filenames)\n    dataset = dataset.map(read_tfrecord, num_parallel_calls=autotune)\n    return dataset\n\ndef decode_image(image, img_size=[256,256,3]):\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = (tf.cast(image, tf.float32) / 127.5) - 1        \n    image = tf.reshape(image, img_size)             \n    return image\n\ndef read_tfrecord(example):\n    tfrecord_format = {\n        \"image_name\": tf.io.FixedLenFeature([], tf.string),\n        \"image\":      tf.io.FixedLenFeature([], tf.string),\n        \"target\":     tf.io.FixedLenFeature([], tf.string)\n    }\n    example = tf.io.parse_single_example(example, tfrecord_format)  \n    image = decode_image(example['image'])    \n    return image\n\ndef count_data_items(filenames):\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)\n\ndata_path = '/kaggle/input/gan-getting-started'\nmonet_filenames = tf.io.gfile.glob(str(os.path.join(data_path, 'monet_tfrec', '*.tfrec')))\nphoto_filenames = tf.io.gfile.glob(str(os.path.join(data_path, 'photo_tfrec', '*.tfrec')))\n\nmonet_ds = load_dataset(monet_filenames)\nphoto_ds = load_dataset(photo_filenames)\n\nn_monet_samples = count_data_items(monet_filenames)\nn_photo_samples = count_data_items(photo_filenames)\ndataset = tf.data.Dataset.zip((monet_ds, photo_ds))\n\nn_monet_samples, n_photo_samples","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plot a few sample images from monet style and photo images.","metadata":{}},{"cell_type":"code","source":"def plot_images(images, title):\n    plt.figure(figsize=(15,15))\n    plt.subplots_adjust(0,0,1,0.95,0.05,0.05)\n    j = 1\n    for i in np.random.choice(len(images), 100, replace=False):\n        plt.subplot(10,10,j), plt.imshow(images[i] / images[i].max()), plt.axis('off')\n        j += 1\n    plt.suptitle(title, size=25)\n    plt.show()\n\nmonet_numpy = list(monet_ds.as_numpy_iterator())\nplot_images(monet_numpy, 'Monet images')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Monet Style input images","metadata":{}},{"cell_type":"code","source":"plot_images(list(photo_ds.as_numpy_iterator()), 'Photo images')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing\n\nThe images are transformed to have values in between [-1,1], implemented with the function `decode_image()` above. This is what I did and found to be the most common form of pre processing. I reviewed the top scored public code and saw they use something called a CLIP model to extract features during the transformation process. This is a pre trained model used. This is not something I implemented here but discuss in my future work section. \n\n","metadata":{}},{"cell_type":"markdown","source":"## Model Architecture","metadata":{}},{"cell_type":"markdown","source":"Paired data is harder to find in most domains and not something we have here which makes the unsupervised training capabilities of **CycleGAN** quite useful, it does not require paired training data. The problem can be forumulated as unpaired image-to-image translation and CycleGAN is an ideal model to be used here. We shall train the CycleGAN model on the image dataset provided and then use the Genrator to generate monet images later.","metadata":{}},{"cell_type":"code","source":"class CycleGAN(Model):  # Define CycleGAN inheriting from Model\n    def __init__(\n        self,\n        monet_generator,\n        photo_generator,\n        monet_discriminator,\n        photo_discriminator,\n        lambda_cycle=10,\n    ):\n        super(CycleGAN, self).__init__()\n        self.m_gen = monet_generator\n        self.p_gen = photo_generator\n        self.m_disc = monet_discriminator\n        self.p_disc = photo_discriminator\n        self.lambda_cycle = lambda_cycle\n        \n    def compile(\n        self,\n        m_gen_optimizer,\n        p_gen_optimizer,\n        m_disc_optimizer,\n        p_disc_optimizer,\n        gen_loss_fn,\n        disc_loss_fn,\n        cycle_loss_fn,\n        identity_loss_fn\n    ):\n        super(CycleGAN, self).compile()\n        self.m_gen_optimizer = m_gen_optimizer\n        self.p_gen_optimizer = p_gen_optimizer\n        self.m_disc_optimizer = m_disc_optimizer\n        self.p_disc_optimizer = p_disc_optimizer\n        self.gen_loss_fn = gen_loss_fn\n        self.disc_loss_fn = disc_loss_fn\n        self.cycle_loss_fn = cycle_loss_fn\n        self.identity_loss_fn = identity_loss_fn\n\n    def generate(self, image):\n        return self.m_gen(tf.expand_dims(image, axis=0), training=False)\n\n    def load(\n        self, \n        filepath\n    ):\n        self.m_gen.load_weights(filepath.replace('model_name', 'm_gen'), by_name=True)\n        self.p_gen.load_weights(filepath.replace('model_name', 'p_gen'), by_name=True)\n        self.m_disc.load_weights(filepath.replace('model_name', 'm_disc'), by_name=True)\n        self.p_disc.load_weights(filepath.replace('model_name', 'p_disc'), by_name=True)\n\n    def save(\n        self, \n        filepath\n    ):\n        self.m_gen.save(filepath.replace('model_name', 'm_gen'))\n        self.p_gen.save(filepath.replace('model_name', 'p_gen'))\n        self.m_disc.save(filepath.replace('model_name', 'm_disc'))\n        self.p_disc.save(filepath.replace('model_name', 'p_disc'))\n\n        \n    def train_step(self, batch_data):\n        real_monet, real_photo = batch_data\n        \n        with tf.GradientTape(persistent=True) as tape:\n            # photo to monet back to photo\n            real_photo = tf.expand_dims(real_photo, axis=0)\n            real_monet = tf.expand_dims(real_monet, axis=0)\n\n            fake_monet = self.m_gen(real_photo, training=True)\n            cycled_photo = self.p_gen(fake_monet, training=True)\n\n            # monet to photo back to monet\n            fake_photo = self.p_gen(real_monet, training=True)\n            cycled_monet = self.m_gen(fake_photo, training=True)\n\n            # generating itself\n            same_monet = self.m_gen(real_monet, training=True)\n            same_photo = self.p_gen(real_photo, training=True)\n\n            # discriminator used to check, inputing real images\n            disc_real_monet = self.m_disc(real_monet, training=True)\n            disc_real_photo = self.p_disc(real_photo, training=True)\n\n            # discriminator used to check, inputing fake images\n            disc_fake_monet = self.m_disc(fake_monet, training=True)\n            disc_fake_photo = self.p_disc(fake_photo, training=True)\n\n            # evaluates generator loss\n            monet_gen_loss = self.gen_loss_fn(disc_fake_monet)\n            photo_gen_loss = self.gen_loss_fn(disc_fake_photo)\n\n            # evaluates total cycle consistency loss\n            total_cycle_loss = self.cycle_loss_fn(real_monet, cycled_monet, self.lambda_cycle) + self.cycle_loss_fn(real_photo, cycled_photo, self.lambda_cycle)\n\n            # evaluates total generator loss\n            total_monet_gen_loss = monet_gen_loss + total_cycle_loss + self.identity_loss_fn(real_monet, same_monet, self.lambda_cycle)\n            total_photo_gen_loss = photo_gen_loss + total_cycle_loss + self.identity_loss_fn(real_photo, same_photo, self.lambda_cycle)\n\n            # evaluates discriminator loss\n            monet_disc_loss = self.disc_loss_fn(disc_real_monet, disc_fake_monet)\n            photo_disc_loss = self.disc_loss_fn(disc_real_photo, disc_fake_photo)\n\n        # Calculate the gradients for generator and discriminator\n        monet_generator_gradients = tape.gradient(total_monet_gen_loss, self.m_gen.trainable_variables)\n        photo_generator_gradients = tape.gradient(total_photo_gen_loss, self.p_gen.trainable_variables)\n        monet_discriminator_gradients = tape.gradient(monet_disc_loss, self.m_disc.trainable_variables)\n        photo_discriminator_gradients = tape.gradient(photo_disc_loss, self.p_disc.trainable_variables)\n\n        # Apply the gradients to the optimizer\n        self.m_gen_optimizer.apply_gradients(zip(monet_generator_gradients, self.m_gen.trainable_variables))\n        self.p_gen_optimizer.apply_gradients(zip(photo_generator_gradients, self.p_gen.trainable_variables))\n        self.m_disc_optimizer.apply_gradients(zip(monet_discriminator_gradients, self.m_disc.trainable_variables))\n        self.p_disc_optimizer.apply_gradients(zip(photo_discriminator_gradients, self.p_disc.trainable_variables))\n        \n        total_loss = total_monet_gen_loss + total_photo_gen_loss + monet_disc_loss + photo_disc_loss\n\n        return {\n            \"total_loss\": total_loss,\n            \"monet_gen_loss\": total_monet_gen_loss,\n            \"photo_gen_loss\": total_photo_gen_loss,\n            \"monet_disc_loss\": monet_disc_loss,\n            \"photo_disc_loss\": photo_disc_loss\n        }\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CycleGAN Architecture\n\nThe CycleGAN is an extension of the GAN architecture designed for image-to-image translation tasks. Unlike the standard GAN, which involves a single generator and discriminator, the CycleGAN architecture involves the simultaneous training of two generator models and two discriminator models.\n\n### Cycle Consistency Loss\nThis is the core idea that distinguishes CycleGAN from other GANs. It ensures that an image translated to the target domain and then back to the source domain should match the original image. This is implemented by introducing a *cycle* consistency loss:\n\n**Forward Cycle Consistency Loss**: Ensures that translating an image from Domain A (photos) to Domain B (Monet paintings) and back to Domain A reconstructs the original image.\n\n**Backward Cycle Consistency Loss**: Ensures that translating an image from Domain B to Domain A and back to Domain B reconstructs the original image.\n\n## Discriminator Models\nTwo discriminator models are used:\n**Discriminator A**: Differentiates between real images from Domain A and fake images generated by the generator from Domain B to Domain A.\n**Discriminator B**: Differentiates between real images from Domain B and fake images generated by the generator from Domain A to Domain B.\n\n## Generator Models\nTwo generator models are used:\n**Generator A to B**: Transforms images from Domain A (photos) to Domain B (Monet paintings).\n**Generator B to A**: Transforms images from Domain B (Monet paintings) to Domain A (photos).\n\n## Model Architecture\n**Generators**: Typically use an encoder-decoder or U-Net architecture. The encoder downsamples the input image to a bottleneck layer, and the decoder upsamples it back to the original size. The use of residual blocks (ResNet layers) with skip connections is common to help preserve image details.\n\n**Discriminators**: Deep convolutional neural networks (often PatchGANs) that classify whether images are real or fake.\n\n## Training Process\n\n**Discriminators**\nTrained to distinguish real images from fake images generated by the corresponding generator.\n\n**Generators**\nTrained to fool the discriminators and to minimize the cycle consistency loss. The loss function for the generators combines:\n**Adversarial Loss**: Ensures the generated images look similar to the target domain.\n**Cycle Consistency Loss**: Ensures that the translated images can be accurately reconstructed back to the original domain.","metadata":{}},{"cell_type":"code","source":"def Generator(img_shape=[256, 256, 3]):\n    inputs = tf.keras.layers.Input(shape=img_shape)\n    down_stack = [\n        downsample(64, 4, apply_instancenorm=False),\n        downsample(128, 4),\n        downsample(256, 4),\n        downsample(512, 4),\n        downsample(512, 4),\n        downsample(512, 4),\n        downsample(512, 4),\n        downsample(512, 4),\n    ]\n    up_stack = [\n        upsample(512, 4, apply_dropout=True),\n        upsample(512, 4, apply_dropout=True),\n        upsample(512, 4, apply_dropout=True),\n        upsample(512, 4),\n        upsample(256, 4),\n        upsample(128, 4),\n        upsample(64, 4),\n    ]\n\n    initializer = tf.random_normal_initializer(0., 0.02)\n    last = tf.keras.layers.Conv2DTranspose(3, 4, strides=2, padding='same', kernel_initializer=initializer, activation='tanh')\n\n    x = inputs\n    skips = []\n    for down in down_stack:\n        x = down(x)\n        skips.append(x)\n    skips = reversed(skips[:-1])\n\n    for up, skip in zip(up_stack, skips):\n        x = up(x)\n        x = tf.keras.layers.Concatenate()([x, skip])\n    x = last(x)\n    return tf.keras.Model(inputs=inputs, outputs=x)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Discriminator(img_shape=[256, 256, 3]):\n    initializer = tf.random_normal_initializer(0., 0.02)\n    gamma_init = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n\n    inp = tf.keras.layers.Input(shape=img_shape, name='input_image')\n    x = inp\n\n    x = downsample(64, 4, False)(x)\n    x = downsample(128, 4)(x)\n    x = downsample(256, 4)(x)\n\n    x = tf.keras.layers.ZeroPadding2D()(x)\n    x = tf.keras.layers.Conv2D(512, 4, strides=1, kernel_initializer=initializer, use_bias=False)(x)\n    x = tf.keras.layers.LayerNormalization(gamma_initializer=gamma_init)(x)\n    x = tf.keras.layers.LeakyReLU()(x)\n\n    x = tf.keras.layers.ZeroPadding2D()(x)\n    x = tf.keras.layers.Conv2D(1, 4, strides=1, kernel_initializer=initializer)(x)\n\n    return tf.keras.Model(inputs=inp, outputs=x)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def downsample(filters, size, apply_instancenorm=True):\n    initializer = tf.random_normal_initializer(0., 0.02)\n    gamma_init = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n    \n    result = tf.keras.Sequential()\n    result.add(tf.keras.layers.Conv2D(filters, size, strides=2, padding='same', kernel_initializer=initializer, use_bias=False))\n    if apply_instancenorm:\n        result.add(tf.keras.layers.LayerNormalization(gamma_initializer=gamma_init))\n    result.add(tf.keras.layers.LeakyReLU())\n    \n    return result\n\n\ndef upsample(filters, size, apply_dropout=False):\n    initializer = tf.random_normal_initializer(0., 0.02)\n    gamma_init = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n    result = tf.keras.Sequential()\n    result.add(tf.keras.layers.Conv2DTranspose(filters, size, strides=2, padding='same', kernel_initializer=initializer, use_bias=False))\n    result.add(tf.keras.layers.LayerNormalization(gamma_initializer=gamma_init))\n    if apply_dropout:\n        result.add(tf.keras.layers.Dropout(0.5))\n    result.add(tf.keras.layers.ReLU())\n    return result\n\ndef discriminator_loss(real, generated):\n    real_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(real), real)\n    generated_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.zeros_like(generated), generated)\n    total_disc_loss = real_loss + generated_loss\n    return total_disc_loss * 0.5\n    \ndef generator_loss(generated):\n    return tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(generated), generated)\n\n        \ndef calc_cycle_loss(real_image, cycled_image, LAMBDA):\n    return LAMBDA * tf.reduce_mean(tf.abs(real_image - cycled_image))\n\ndef identity_loss(real_image, same_image, LAMBDA):\n    return LAMBDA * 0.5 * tf.reduce_mean(tf.abs(real_image - same_image))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_shape = [256, 256, 3]\n\nwith strategy.scope():\n    model = CycleGAN(\n        monet_generator=Generator(img_shape),\n        photo_generator=Generator(img_shape),\n        monet_discriminator=Discriminator(img_shape),\n        photo_discriminator=Discriminator(img_shape),\n        lambda_cycle=10\n    )\n\n    model.compile(\n        m_gen_optimizer=m_gen_optimizer,\n        p_gen_optimizer=p_gen_optimizer,\n        m_disc_optimizer=m_disc_optimizer,\n        p_disc_optimizer=p_disc_optimizer,\n        gen_loss_fn=generator_loss,\n        disc_loss_fn=discriminator_loss,\n        cycle_loss_fn=calc_cycle_loss,\n        identity_loss_fn=identity_loss\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Results and Analysis","metadata":{}},{"cell_type":"markdown","source":"Let's train the GAN model both the generator and discrimnator for 20 epochs. We expect to see the generator and discriminator \"compete\" with the discriminator attempting to correctly classify real images from generated images. The generator attempts to learn how to translate images from one domain (e.g., photos) to another domain (e.g., Monet paintings) and vice versa. In an ideal setting, the discriminator should have a hard time distinguishing real images from generated images, meaning the discriminator's loss should not necessarily be zero. If the discriminator's loss were zero, it would indicate that it perfectly distinguishes real from generated images, implying that the generator is not producing realistic images. Conversely, the generator should be able to produce images that are indistinguishable from real images, achieving a balance where the discriminator is unable to easily classify real versus generated images.\n\nFrom what I see in other notebooks submitted, more epochs performs better but my machine is very slow training this model with each epoch taking about 20 minutes to be run. A batch size of 32 has been chosen to try and reduce the training time. ","metadata":{}},{"cell_type":"code","source":"# Train the model\nbatch_size = 32\nepochs = 20\nhistory = model.fit(dataset, epochs=epochs, batch_size=batch_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Save the model.","metadata":{}},{"cell_type":"code","source":"model.save('cyclegan_SV.h5')","metadata":{"id":"PgXm6a12uQmZ","outputId":"fdfb8b1b-b90f-4c3d-ec1c-3dfa1b22878d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plot the loss for the generators and discriminators, for generators we plot the mean loss.","metadata":{}},{"cell_type":"code","source":"history.history.keys()","metadata":{"id":"PSfmkHBjt-TC","outputId":"c560ecff-391a-4f5a-b029-87c21cc639e2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_hist(hist):\n    monet_gen_loss = np.array(hist.history['monet_gen_loss'][0][0])\n    photo_gen_loss = np.array(hist.history['photo_gen_loss'][0][0])\n    \n    plt.figure(figsize=(12, 6))\n    \n    plt.subplot(121)\n    plt.plot(monet_gen_loss.flatten())\n    plt.plot(photo_gen_loss.flatten())\n    plt.legend([\"monet\", \"photo\"])\n    plt.xlabel('Epoch')\n    plt.ylabel('Generator Loss')\n    plt.title('Generator Loss over Epochs')\n    \n    plt.subplot(122)\n    monet_disc_loss = np.array(hist.history['monet_disc_loss'][0][0])\n    photo_disc_loss = np.array(hist.history['photo_disc_loss'][0][0])\n    plt.plot(monet_disc_loss.flatten())\n    plt.plot(photo_disc_loss.flatten())\n    plt.legend([\"monet\", \"photo\"])\n    plt.xlabel('Epoch')\n    plt.ylabel('Discriminator Loss')\n    plt.title('Discriminator Loss over Epochs')\n    \n    plt.tight_layout()\n    plt.show()\n\n# Assuming 'history' is the result of model.fit\nplot_hist(history)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, let's use the generated to generate ~7k images and save / submit the notebook to kaggle.","metadata":{}},{"cell_type":"code","source":"! mkdir ../images\n\ndef generate(dataset):\n    dataset_iter = iter(dataset)\n    out_dir = '../images/'\n    for i in tqdm.tqdm(range(n_photo_samples)):\n        # Get the image from the dataset iterator\n        img = next(dataset_iter)\n        prediction = model.generate(img)\n        prediction = tf.squeeze(prediction).numpy()\n        prediction = (prediction * 127.5 + 127.5).astype(np.uint8)   \n        plt.imsave(os.path.join(out_dir, 'image_{:04d}.jpg'.format(i)), prediction)\n\ngenerate(photo_ds)\n\nshutil.make_archive(\"/kaggle/working/images\", 'zip', \"/kaggle/images\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion and Future Work\n\nThe CycleGAN does a pretty good job in generating the monet-style images from photos. The model was trained for 25 epochs, it seems that losses will decrease further if increased for more epochs (e.g., 100), we can get a better model. However, my machine is very slow at training this model with it taking roughly 5-8 hours depending on the amount of epochs I use. \n\nWith this in mind, future work would include optimizing the code to run faster and attempting to implement a strategy that utilzied GPU and/or parellel computing. At this time, I have spent significant time on this model to get it to this point and need to move on but I hope to learn more about these methods and attempt to implement them in the future. I think improving the computation of the code will have the largest impact as from what I have read in other notebooks and online, being able to tune the model depends on the computing power of the machine. For instance, being able to run more epochs. \n\nKaggle uses an evaluation metric called MiFID (Memorization-informed Fréchet Inception Distance) score to evaluate the quality of generated images.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}